{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing Guide for Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we explain how the datasets for SIIM-ISIC and COCO-Stuff were trimmed down. This was done due to the large size of the original datasets, which made storing the data harder. As such, below we provide the scripts used to trim down both these datasets.\n",
    "\n",
    "As mentioned in the other files, the trimmed down datasets have been uploaded to Hugging Face and can be downloaded via the prepared scripts (which can be found [here](../scripts/download_cocostuff) and [here](../scripts/download_siim)).\n",
    "\n",
    "Before starting, however, we of course have to begin with dependency imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **COCO-Stuff**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to how the original dataset was initially setup for multi-object classification, the processing required is somewhat complex by nature. An overview of such processing would be:\n",
    "\n",
    "1. Select all images containing the target label for each label.\n",
    "2. Group all the labels based on the image containing it and export as a JSON dict file.\n",
    "3. Obtain all the unique directories required as an image can have multiple class labels in a list.\n",
    "4. Save all the unique images obtained in a new folder.\n",
    "\n",
    "This should obtain the same dataset as in the link, assuming all the files are in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we first define helper functions to ease the processing\n",
    "def idx2img(json_file, image_dir):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    json_file: the loaded dict originating from the annotation JSON file\n",
    "    image_dir: the directory containing all the images (either `train2017` or `val2017`)\n",
    "    \n",
    "    Outputs:\n",
    "    img_dict: dictionary containing mappings of '{<image index>: image_dir + <image filename>}'\n",
    "    \"\"\"\n",
    "    img_dict = {}\n",
    "    for data in json_file['images']:\n",
    "\n",
    "        id = data['id']\n",
    "        name = data['file_name']\n",
    "        img_dict[id] = os.path.join(image_dir, name)\n",
    "\n",
    "    return img_dict\n",
    "\n",
    "\n",
    "def clean_data(annot_dir, image_dir):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    annot_dir: the annotation JSON file path (from `annotations_trainval2017.zip` as the authors here \n",
    "               appear to focus more on `things` annotations)\n",
    "    image_dir: the directory containing all the images (either `train2017` or `val2017`)\n",
    "    \n",
    "    Outputs:\n",
    "    img_idx_dict: dictionary containing mappings of '{image_dir + <image filename>: <image index list>}'\n",
    "    \"\"\"\n",
    "    f = open(annot_dir)\n",
    "    data = json.load(f)\n",
    "    img_dict = idx2img(data, image_dir)\n",
    "\n",
    "    # even split between training and test data\n",
    "    img_idx_dict = {}\n",
    "\n",
    "    # extract the data\n",
    "    for segment in data['annotations']:\n",
    "        class_id = segment['category_id']\n",
    "        image_id = img_dict[segment['image_id']]\n",
    "        if image_id not in img_idx_dict.keys():\n",
    "            img_idx_dict[image_id] = [class_id]\n",
    "        elif class_id not in img_idx_dict[image_id]:\n",
    "            img_idx_dict[image_id] += [class_id]\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return img_idx_dict\n",
    "\n",
    "# Note: this function is similar to the one used in `coco_stuff.py`\n",
    "def load_coco_data(annot_dict, target:int, n_samples:int=500,\n",
    "                   seed:int=42):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    annot_dict: the loaded dict originating from the annotation JSON file\n",
    "    target: the target class index for the dataset (see `./data/coco_target_indexes.txt` for reference)\n",
    "    n_samples: the number of samples to take (ideally an even number)\n",
    "    seed: the random seed which determines the samples taken\n",
    "    \n",
    "    Outputs:\n",
    "    datalist: list of filenames to take\n",
    "    \"\"\"\n",
    "    # set the random seed for numpy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # even split between training and test data\n",
    "    npc = n_samples // 2\n",
    "        \n",
    "    # select images based on whether not the image has a segmentation of the target object\n",
    "    pos_data = []\n",
    "    neg_data = []\n",
    "    for imgc in annot_dict.keys():\n",
    "        if target in annot_dict[imgc]:\n",
    "            pos_data.append(imgc)\n",
    "\n",
    "        else:\n",
    "            neg_data.append(imgc)\n",
    "\n",
    "    # sample with replacement if too little data is present    \n",
    "    replace_pos = True if (len(pos_data) < npc) else False\n",
    "    replace_neg = True if (len(neg_data) < npc) else False\n",
    "    pos_sample = np.random.choice(pos_data, size=npc, replace=replace_pos)\n",
    "    neg_sample = np.random.choice(neg_data, size=npc, replace=replace_neg)\n",
    "\n",
    "    datalist = list(np.concatenate((pos_sample, neg_sample), axis=None))\n",
    "\n",
    "    return datalist\n",
    "\n",
    "# this function is just in case there are discrepancies between the filenames present\n",
    "def sync_data(json_dict:dict, filelist:list):\n",
    "    \n",
    "    final_dict = {}\n",
    "    for file in filelist:\n",
    "        if file in json_dict.keys():\n",
    "            final_dict[file] = json_dict[file]\n",
    "    \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is separated because loading the JSON files takes a bit of time\n",
    "train_json = clean_data(\"annotations/instances_train2017.json\", \"train2017\")\n",
    "test_json = clean_data(\"annotations/instances_val2017.json\", \"val2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we take all the needed files for training and test (from each folder)\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "targets = [3, 6, 31, 35, 36, 37, 40, 41, 43, 46, 47, 50, 53, 64, 75, 76, 78, 80, 85, 89]\n",
    "\n",
    "for target in targets:\n",
    "    data = load_coco_data(train_json, target)\n",
    "    train_files.extend(data)\n",
    "\n",
    "for target in targets:\n",
    "    data = load_coco_data(test_json, target, 250)\n",
    "    test_files.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we remove the duplicates\n",
    "train_files = list(set(train_files))\n",
    "test_files = list(set(test_files))\n",
    "\n",
    "# and add an extra step as a guarantee that all files are the same between objects\n",
    "final_train = sync_data(train_json, train_files)\n",
    "final_test = sync_data(test_json, test_files)\n",
    "\n",
    "# ...and then export!\n",
    "out_tr = open(\"trimmed/labels_train.json\", \"w\") \n",
    "out_ts = open(\"trimmed/labels_val.json\", \"w\") \n",
    "\n",
    "json.dump(final_train, out_tr, indent = 2) \n",
    "json.dump(final_test, out_ts, indent = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for copying the files over to a new folder\n",
    "for filename in train_files:\n",
    "    target = os.path.join('trimmed', filename)\n",
    "    shutil.copyfile(filename, target)\n",
    "\n",
    "for filename in test_files:\n",
    "    target = os.path.join('trimmed', filename)\n",
    "    shutil.copyfile(filename, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SIIM-ISIC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, the process mainly involved just selecting 2000 images from the original and ensuring that all the label splits are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dir = \"./artifacts/data/SIIM_ISIC/metadata.csv\" # path to the original metadata file for SIIM-ISIC\n",
    "df = pd.read_csv(df_dir)\n",
    "df = df.sort_values(by=['benign_malignant'])\n",
    "\n",
    "df_ben = df.loc[df['benign_malignant'] == 'benign'][2000:4000]\n",
    "df_mal = df.loc[df['benign_malignant'] == 'malignant'][50:550]\n",
    "full_df = pd.concat([df_ben, df_mal])\n",
    "\n",
    "target_dir = \"data\"\n",
    "\n",
    "full_df.to_csv(\"metadata_small.csv\", index=False) # here we have a different filename to prevent overwriting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "PCBM": "PCBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
