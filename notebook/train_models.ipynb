{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NZu3LuJt-YB"
      },
      "source": [
        "# **Post-Hoc Concept Bottleneck Models Replication**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This file assumes that the main README instructions have already been followed, which would be every step after the environment has been activated. If not, then you can view it [here](../README.md).\n",
        "\n",
        "The instructions in this notebook parallel those present in the README in the later steps, meaning that it is possible to follow either for guidance.\n",
        "\n",
        "Before starting, all the necessary files need to first be prepared. This notebook, when run, will setup all the necessary installations in the environment. We need to first move outside of the `\\notebook` directory via the code block below. It should automatically setup the directory depending on whether this notebook is being run locally or on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "REBF9Gh8t-YC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Directory:\n",
            "c:\\Users\\Gregory Go\\.github\\FACT\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    url = f\"https://{userdata.get('gh_pat')}@github.com/dgcnz/FACT.git\" # Note: may need to be scrubbed\n",
        "    !git clone -b \"greg\" --single-branch {url} # Change this back\n",
        "    !pip install git+https://github.com/openai/CLIP.git # for the CLIP library\n",
        "    print(\"\\nCurrent Directory:\")\n",
        "    %cd FACT\n",
        "\n",
        "else: # automatically checks if the current directory is 'FACT'\n",
        "    curdir = os.getcwd()\n",
        "    curdir = curdir.split('\\\\')[-1]\n",
        "    if curdir != \"FACT\":\n",
        "        print(\"Current Directory:\")\n",
        "        %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should now be in the **FACT** main folder. This is important for running the files to ensure that they save/search in the correct locations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Downloading the Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BRODEN Concepts Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Note_: There is a potential permission error which may arise when trying to download the files via this notebook. Manual downloading may be needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHOz6uLCt-YF",
        "outputId": "3dbc5173-f488-404d-e0bd-27637385eb16"
      },
      "outputs": [],
      "source": [
        "# Get the BRODEN concepts dataset\n",
        "!bash ./scripts/download_broden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## COCO-Stuff Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Note:* The dataset is around 20 GB in total. Ensure you have enough space on your device before attempting to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEm-yldNt-YF",
        "outputId": "69ecc50e-dc6c-44e9-bf6d-8d93ed774807"
      },
      "outputs": [],
      "source": [
        "# Get the COCO-stuff dataset (bash is needed to run the command below)\n",
        "!bash ./scripts/download_cocostuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the CUB dataset (bash is needed here to run the command below)\n",
        "!bash ./scripts/download_cub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Derm7pt Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Note:_ Due to how the download has been setup by the Original Derm7pt authors, registering to their site is necessary to access the dataset, thus meaning that we need to perform some manual processing. As such, please perform the following steps:\n",
        "\n",
        "1. Go to the Derm7pt site [here](https://derm.cs.sfu.ca/Download.html).\n",
        "2. Fill in the form with the necessary details.\n",
        "3. The email received should contain the download link alongside the needed login credentials below it. Click the link and then fill in the details in the prompt given, which should automatically trigger the download afterwards.\n",
        "4. Extract the .zip file and rename the folder extracted to `derm7pt`.\n",
        "5. Move this folder to `./FACT/artifacts/data`.\n",
        "\n",
        "_Note:_ If desired, for Google Colab you can upload the dataset to Google Drive and copy it to the current session using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set the location variable to the specific directory in your Google Drive\n",
        "location = \"path/to/your/directory/in/drive\"\n",
        "\n",
        "# construct the source and destination paths\n",
        "source_path = f\"/content/drive/MyDrive/{location}\"\n",
        "destination_path = \"/content/FACT/artifacts/data\"\n",
        "\n",
        "# copy to the destination path\n",
        "!cp -r \"{source_path}\" \"{destination_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HAM10000 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Note:_ The HAM10000 Dataset is made available as a public Kaggle dataset. In order to download it through this script, make sure you have a Kaggle API token ready and place it (a .JSON file) in the following directory: `C:\\Users\\\\*your username*\\\\.kaggle`.\n",
        "\n",
        "To create a Kaggle API token, please do the following steps:\n",
        "\n",
        "1. Go to your [account settings](https://www.kaggle.com/account). You will need to create a Kaggle account if you do not have one already.\n",
        "2. Click on your profile icon > \"Settings\" > Scroll down to \"API\" > click \"Create New Token\"\n",
        "3. This will download a file named `kaggle.json`. Again remember to move it to the scripts folder in the **FACT** directory.\n",
        "\n",
        "If on Google Colab, upload your generated API token (`kaggle.json`) to any folder you want and paste the directory to that file to the `folder_containing_api` variable below.\n",
        "\n",
        "Afterwards, just run the following codeblock:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the HAM10K dataset (bash is needed here to run the command below)\n",
        "\n",
        "folder_containing_api = \"\" # Add your folder here if on Colab\n",
        "\n",
        "if IN_COLAB:\n",
        "    import os\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = f'/content/drive/MyDrive/{folder_containing_api}'\n",
        "\n",
        "!bash ./scripts/download_ham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SIIM-ISIC Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Note_: The original dataset is around 23 GB in total. The version downloaded by this script is a trimmed-down version which replicates what the original authors did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the SIIM-ISIC dataset\n",
        "!python ./scripts/download_siim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metashift Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Note_: The original dataset is around __ GB in total. The version downloaded by this script is a trimmed-down version which replicates what the original authors did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the Metashift dataset\n",
        "!bash ./scripts/download_metashift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and that would be every dataset needed for reproducing the main results! If you'd like, you can also download the datasets and dependencies for the extension experiments (which totals ~7 GB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ESC-50 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the ESC-50 Dataset\n",
        "!bash ./scripts/download_esc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## UrbanSound8K Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the HAM10000 Dataset, you need to have an API token ready. Follow the instructions [there](#ham10000-dataset) if you don't have one ready and would like guidance on how to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the UrbanSound8K Dataset\n",
        "\n",
        "folder_containing_api = \"\" # Add your folder here if on Colab\n",
        "\n",
        "if IN_COLAB:\n",
        "    import os\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = f'/content/drive/MyDrive/{folder_containing_api}'\n",
        "\n",
        "!bash ./scripts/download_us8k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AudioCLIP Dependencies\n",
        "\n",
        "**Please Note:** Due to how everything is setup, running the below script is necessary to run the experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This downloader only includes the fully pretrained AudioCLIP model and the vocabulary in case needed. The reason being that the main repository for AudioCLIP is not designed to be installed as a Python package. As of writing, no `setup.py` files or anything that would work has been implemented, making it not possible to directly install their repo.\n",
        "\n",
        "Thus, a copy of it has been integrated here, with the assets separated to prevent bottlenecking this repo. You can find the citation to the original authors [here](../models/AudioCLIP/README.md) and their original repository [here](https://github.com/AndreyGuzhov/AudioCLIP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the AudioCLIP Dependencies\n",
        "!bash ./scripts/download_audioclip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Training and Evaluating PCBMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have prepared all the necessary files, we can now begin with replicating the results obtained. \n",
        "\n",
        "Do note however, that some details for replication are missing, meaning that the results may somewhat differ compared to the original paper (which is also influenced by the hardware differences between experiments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In total, there are three concepts datasets needed for these experiments:\n",
        "1. BRODEN\n",
        "2. CUB\n",
        "3. Derm7pt\n",
        "\n",
        "Here we prepare each of these concepts for later use alongside the corresponding models, starting with the BRODEN ones.\n",
        "\n",
        "_Note:_ If you are on Colab, make sure to install PyTorch Ignite and Visdom first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To install PyTorch Ignite and Visdom\n",
        "if IN_COLAB:\n",
        "    !pip install pytorch-ignite\n",
        "    !pip install visdom\n",
        "    !pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_dataset.py \\\n",
        "  --dataset-name=\"broden\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --C 0.001 0.01 0.1 1.0 10.0 \\\n",
        "  --n-samples=50 \\\n",
        "  --out-dir=artifacts/outdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we move on to the CUB concepts,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_dataset.py \\\n",
        "  --dataset-name=\"cub\" \\\n",
        "  --C 0.001 0.01 0.1 1.0 10.0 \\\n",
        "  --n-samples=50 \\\n",
        "  --out-dir=artifacts/outdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... and finally the Derm7pt concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_dataset.py \\\n",
        "  --dataset-name=\"derm7pt\" \\\n",
        "  --backbone-name=\"ham10000_inception\" \\\n",
        "  --C 0.001 0.01 0.1 1.0 10.0 \\\n",
        "  --n-samples=50 \\\n",
        "  --out-dir=artifacts/outdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Multimodal Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One part of the original paper involves learning concepts automatically by utilizing CLIP embeddings. This has already been implemented by the authors in `learn_multimodal_concepts.py` (though some adjustments for improved readability and extra concept banks for extension studies have been made by us).\n",
        "\n",
        "Because of this, we can run the following snippets directly for each dataset, starting with CIFAR10/100 below.\n",
        "\n",
        "_Note:_ Make sure to change the device to match what you would like/have (by default it assumes _cuda_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"cifar10\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --device=\"cuda\"\\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"cifar100\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --device=\"cuda\" \\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can learn the concepts for COCO-Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"cub\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --device=\"cuda\"\\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below you can find the concept learner snippets for the extension experiments which can be executed, if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: 'audio' here refers to AudioCLIP\n",
        "!python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"esc50\" \\\n",
        "  --backbone-name=\"audio\" \\\n",
        "  --device=\"cuda\"\\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_multimodal.py \\\n",
        "  --out-dir=\"artifacts/multimodal\" \\\n",
        "  --classes=\"us8k\" \\\n",
        "  --backbone-name=\"audio\" \\\n",
        "  --device=\"cuda\"\\\n",
        "  --recurse=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Training PCBMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Note:_ For the COCO-Stuff experiments, please set the `out-dir` to `artifacts/outdir/coco-stuff`or else the folder will be way less organized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clip:RN50_10.0_50.pkl\" \\\n",
        "  --dataset=\"cifar10\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam=2e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --dataset=\"coco_stuff\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --out-dir=artifacts/outdir/coco-stuff \\\n",
        "  --lam=2e-4 \\\n",
        "  --no-print-out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extension Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/multimodal/mmc_audio_esc50_recurse_1.pkl\" \\\n",
        "  --dataset=\"esc50\" \\\n",
        "  --backbone-name=\"audio\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam=2e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/multimodal/mmc_audio_us8k_recurse_1.pkl\" \\\n",
        "  --dataset=\"us8k\" \\\n",
        "  --backbone-name=\"audio\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam=2e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Training PCBM-h's**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_pcbm_h.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clip:RN50_10.0_50.pkl\" \\\n",
        "  --pcbm-path=\"artifacts/outdir/pcbm_cifar10__clipRN50__broden_clipRN50_10__lam:0.0002__alpha:0.99__seed:42.ckpt\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --dataset=\"cifar10\" \\\n",
        "  --num-workers=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train_pcbm_h.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clipRN50_10.0_50.pkl\" \\\n",
        "  --pcbm-path=\"artifacts/outdir/coco-stuff\\pcbm_coco_stuff__clipRN50__broden_clipRN50_10__lam_0.0002__alpha_0.99__seed_42_target_3.ckpt\" \\\n",
        "  --out-dir=artifacts/outdir/coco-stuff \\\n",
        "  --dataset=\"coco_stuff\" \\\n",
        "  --num-workers=4 \\\n",
        "  --no-print-out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extension Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_pcbm_h.py \\\n",
        "  --concept-bank=\"artifacts/multimodal/mmc_audio_esc50_recurse_1.pkl\" \\\n",
        "  --pcbm-path=\"artifacts/outdir/\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --dataset=\"esc50\" \\\n",
        "  --num-workers=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_pcbm_h.py \\\n",
        "  --concept-bank=\"artifacts/multimodal/mmc_audio_esc50_recurse_1.pkl\" \\\n",
        "  --pcbm-path=\"artifacts/outdir/\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --dataset=\"us8k\" \\\n",
        "  --num-workers=4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
