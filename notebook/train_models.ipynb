{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NZu3LuJt-YB"
      },
      "source": [
        "# **Post-Hoc Concept Bottleneck Models Replication**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This file assumes that the README instructions have already been followed. If not, then you can view it [here](../README.md).\n",
        "\n",
        "Before starting, all the necessary files need to first be prepared. This notebook, when run, will setup all the necessary installations in the environment. We need to first move outside of the \"notebooks\" directory via the code block below. It should automatically setup the directory depending on whether this notebook is being run locally or on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "REBF9Gh8t-YC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Gregory Go\\.github\\FACT\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    url = f\"https://{userdata.get('gh_pat')}@github.com/dgcnz/FACT.git\" # Note: may need to be scrubbed\n",
        "    !git clone {url}\n",
        "    !pip install git+https://github.com/openai/CLIP.git # for the CLIP library\n",
        "    %cd FACT\n",
        "\n",
        "else: # automatically checks if the current directory is 'FACT'\n",
        "    curdir = os.getcwd()\n",
        "    curdir = curdir.split('\\\\')[-1]\n",
        "    if curdir != \"FACT\":\n",
        "        %cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should now be in the **FACT** main folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Downloading the Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Broden Concepts Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Note_: There is a potential permission error which may arise when trying to download the files via this notebook. Manual downloading may be needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHOz6uLCt-YF",
        "outputId": "3dbc5173-f488-404d-e0bd-27637385eb16"
      },
      "outputs": [],
      "source": [
        "# Get the BRODEN concepts dataset\n",
        "!python ./scripts/download_broden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## COCO-Stuff Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Note:* The dataset is around 20 GB in total. Ensure you have enough space on your device before attempting to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEm-yldNt-YF",
        "outputId": "69ecc50e-dc6c-44e9-bf6d-8d93ed774807"
      },
      "outputs": [],
      "source": [
        "# Get the COCO-stuff dataset (bash is needed to run the command below)\n",
        "!bash ./scripts/download_cocostuff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the CUB dataset (bash is needed here to run the command below)\n",
        "!bash ./scripts/download_cub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Derm7pt Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to how the download has been setup by the Original Derm7pt authors, registering to their site is necessary to access the dataset, thus meaning that we need to perform some manual processing. As such, please perform the following steps:\n",
        "\n",
        "1. Go to the Derm7pt site [here](https://derm.cs.sfu.ca/Download.html).\n",
        "2. Fill in the form with the necessary details.\n",
        "3. The email received should contain the download link alongside the needed login credentials below it. Click the link and then fill in the details in the prompt given, which should automatically trigger the download afterwards.\n",
        "4. Extract the .zip file and rename the folder extracted to \"derm7pt\".\n",
        "5. Move this folder to \"./FACT/artifacts/data\".\n",
        "\n",
        "_Note:_ If desired, for Google Colab you can upload the dataset to Google Drive and copy it to the current session using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HAM10000 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The HAM10000 Dataset is made available as a public Kaggle dataset. In order to download it through this script, make sure you have a Kaggle API token ready and place it in the \"scripts\" folder.\n",
        "\n",
        "To create a Kaggle API token, please do the following steps:\n",
        "\n",
        "1. Go to your [account settings](https://www.kaggle.com/account). You will need to create a Kaggle account if you do not have one already.\n",
        "2. Click on your profile icon > \"Settings\" > Scroll down to \"API\" > click \"Create New Token\"\n",
        "3. This will download a file named kaggle.json. Again remember to move it to the scripts folder in the **FACT** directory.\n",
        "\n",
        "Afterwards, just run the following codeblock:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the HAM10K dataset (bash is needed here to run the command below)\n",
        "!bash ./scripts/download_ham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SIIM-ISIC Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Note_: The original dataset is around 23 GB in total. The version downloaded by this script is a trimmed-down version which replicates what the original authors did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the SIIM-ISIC dataset\n",
        "!python ./scripts/download_siim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Training and Evaluating PCBMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have prepared all the necessary files, we can now begin with replicating the results obtained. \n",
        "\n",
        "Do note however, that some details for replication are missing, meaning that the results may somewhat differ compared to the original paper (which is also influenced by the hardware differences between experiments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Learning Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In total, there are three concepts datasets needed for these experiments:\n",
        "1. BRODEN\n",
        "2. CUB\n",
        "3. Derm7pt\n",
        "\n",
        "Here we prepare each of these concepts for later use alongside the corresponding models, starting with the BRODEN ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_dataset.py \\\n",
        "  --dataset-name=\"broden\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --C 0.001 0.01 0.1 1.0 10.0 \\\n",
        "  --n-samples=50 \\\n",
        "  --out-dir=artifacts/outdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the CUB concepts,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_dataset.py \\\n",
        "  --dataset-name=\"cub\" \\\n",
        "  --C 0.001 0.01 0.1 1.0 10.0 \\\n",
        "  --n-samples=50 \\\n",
        "  --out-dir=artifacts/outdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... and finally the Derm7pt concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python learn_concepts_dataset.py \\\n",
        "  --dataset-name=\"derm7pt\" \\\n",
        "  --backbone-name=\"ham10000_inception\" \\\n",
        "  --C 0.001 0.01 0.1 1.0 10.0 \\\n",
        "  --n-samples=50 \\\n",
        "  --out-dir=artifacts/outdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training PCBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_pcbm.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clip:RN50_10.0_50.pkl\" \\\n",
        "  --dataset=\"cifar10\" \\\n",
        "  --backbone-name=\"clip:RN50\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --lam=2e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training PCBM-h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_pcbm_h.py \\\n",
        "  --concept-bank=\"artifacts/outdir/broden_clip:RN50_10.0_50.pkl\" \\\n",
        "  --pcbm-path=\"artifacts/outdir/pcbm_cifar10__clip:RN50__broden_clip:RN50_10__lam:0.0002__alpha:0.99__seed:42.ckpt\" \\\n",
        "  --out-dir=artifacts/outdir \\\n",
        "  --dataset=\"cifar10\" \\\n",
        "  --num-workers=4\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
